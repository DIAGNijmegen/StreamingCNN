{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StreamingCNN\n",
    "\n",
    "To train deep convolutional neural networks, the input data and the activations need to be kept in memory. Given the limited memory available in current GPUs, this limits the maximum dimensions of the input data. StreamingCNN allows for training a convolutional neural networks while holding only parts of the image in memory. \n",
    "\n",
    "**This notebook shows numerical equivalence to a conventional forward and backward pass.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:36.384174Z",
     "start_time": "2019-11-08T15:07:35.663176Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from scnn import StreamingCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:36.389332Z",
     "start_time": "2019-11-08T15:07:36.385829Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize an small example network here. All layers are supported, except for feature-wide operations (BatchNormalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:36.406848Z",
     "start_time": "2019-11-08T15:07:36.391150Z"
    }
   },
   "outputs": [],
   "source": [
    "padding = 0\n",
    "\n",
    "stream_net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 16, kernel_size=3, padding=padding), torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(16, 16, kernel_size=3, padding=padding), torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(2),\n",
    "    torch.nn.Conv2d(16, 16, kernel_size=3, padding=padding), torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(16, 16, kernel_size=3, padding=padding), torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(2),\n",
    "    torch.nn.Conv2d(16, 16, kernel_size=3, padding=padding), torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(16, 16, kernel_size=3, padding=padding), torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enlarge the weights a bit to increase the gradient sizes (better for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:36.411882Z",
     "start_time": "2019-11-08T15:07:36.408857Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, layer in enumerate(stream_net.modules()):\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        layer.weight.data *= 2.5\n",
    "        layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:36.418976Z",
     "start_time": "2019-11-08T15:07:36.414013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (6): ReLU()\n",
      "  (7): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (8): ReLU()\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (11): ReLU()\n",
      "  (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (13): ReLU()\n",
      "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(stream_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:36.431330Z",
     "start_time": "2019-11-08T15:07:36.422710Z"
    }
   },
   "outputs": [],
   "source": [
    "tile_size = 512\n",
    "img_size = 1024\n",
    "\n",
    "cuda = True  # execute this notebook on the GPU\n",
    "verbose = True   # enable / disable logging\n",
    "dtype = torch.double  # test with double precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:39.681141Z",
     "start_time": "2019-11-08T15:07:36.434031Z"
    }
   },
   "outputs": [],
   "source": [
    "stream_net.type(dtype)\n",
    "if cuda: stream_net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure StreamingCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#FF000'>**IMPORTANT:**</font> setting ```gather_gradients``` to ```True``` makes the class save all the gradients of the intermediate feature maps. This is needed because we want to compare the feature map gradients between streaming and conventional backpropagation. However this also counteracts the memory gains by StreamingCNN. If you want to test the memory efficiency, set ```gather_gradients``` to ```False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:40.502606Z",
     "start_time": "2019-11-08T15:07:39.683395Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " (Lost top:0.0 left:0.0 bottom:0.0 right:0.0)\n",
      "\n",
      " Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " (Lost top:0.0 left:0.0 bottom:0.0 right:0.0)\n",
      "\n",
      " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) \n",
      " (Lost top:0.0 left:0.0 bottom:0.0 right:0.0)\n",
      "\n",
      " Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " (Lost top:0.0 left:0.0 bottom:0.0 right:0.0)\n",
      "\n",
      " Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " (Lost top:0.0 left:0.0 bottom:0.0 right:0.0)\n",
      "\n",
      " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) \n",
      " (Lost top:0.0 left:0.0 bottom:0.0 right:0.0)\n",
      "\n",
      " Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " (Lost top:0.0 left:0.0 bottom:0.0 right:0.0)\n",
      "\n",
      " Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " (Lost top:0.0 left:0.0 bottom:0.0 right:0.0)\n",
      "\n",
      " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) \n",
      " (Lost top:0.0 left:0.0 bottom:0.0 right:0.0)\n",
      "\n",
      " Output lost (Lost top:0.0 left:0.0 bottom:0.0 right:0.0)\n",
      "\n",
      "\n",
      " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) \n",
      " (Lost top:0.0 left:0.0 bottom:0.0 right:0.0)\n",
      "\n",
      " Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " (Lost top:0.0 left:0.0 bottom:1.0 right:1.0)\n",
      "\n",
      " Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " (Lost top:2.0 left:2.0 bottom:3.0 right:3.0)\n",
      "\n",
      " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) \n",
      " (Lost top:4.0 left:4.0 bottom:5.0 right:5.0)\n",
      "\n",
      " Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " (Lost top:8.0 left:8.0 bottom:10.0 right:10.0)\n",
      "\n",
      " Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " (Lost top:10.0 left:10.0 bottom:12.0 right:12.0)\n",
      "\n",
      " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) \n",
      " (Lost top:12.0 left:12.0 bottom:14.0 right:14.0)\n",
      "\n",
      " Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " (Lost top:24.0 left:24.0 bottom:28.0 right:28.0)\n",
      "\n",
      " Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1)) \n",
      " (Lost top:26.0 left:26.0 bottom:30.0 right:30.0)\n",
      "\n",
      " Input gradient lost (Lost top:28.0 left:28.0 bottom:32.0 right:32.0)\n"
     ]
    }
   ],
   "source": [
    "sCNN = StreamingCNN(stream_net, \n",
    "                    tile_shape=(1, 3, tile_size, tile_size), \n",
    "                    verbose=True, \n",
    "                    gather_gradients=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the ```verbose``` flag is ```True``` than StreamingCNN will print for every layer in the network the required overlap that is needed to reconstruct the feature maps and gradients. The higher this is, the more tiles are needed to be inferences. It is always beneficial to increase the tile size as much as possible to make use of all the GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generate random image and fake label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:40.550783Z",
     "start_time": "2019-11-08T15:07:40.504817Z"
    }
   },
   "outputs": [],
   "source": [
    "image = torch.FloatTensor(3, img_size, img_size).normal_(0, 1)\n",
    "target = torch.tensor(50.)  # large value so we get larger gradients\n",
    "\n",
    "image = image.type(dtype)\n",
    "target = target.type(dtype)\n",
    "\n",
    "if cuda:\n",
    "    target = target.cuda()\n",
    "    image = image.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:40.554447Z",
     "start_time": "2019-11-08T15:07:40.552045Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run through network using streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:41.253803Z",
     "start_time": "2019-11-08T15:07:40.555823Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5616595557, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_output = sCNN.forward(image); stream_output.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:41.257403Z",
     "start_time": "2019-11-08T15:07:41.255144Z"
    }
   },
   "outputs": [],
   "source": [
    "stream_output.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:41.263939Z",
     "start_time": "2019-11-08T15:07:41.259009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8238510002, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.sigmoid(torch.mean(stream_output)); output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:41.270191Z",
     "start_time": "2019-11-08T15:07:41.265537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-75.3965480581, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(output, target); loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:41.274133Z",
     "start_time": "2019-11-08T15:07:41.271619Z"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:52.883831Z",
     "start_time": "2019-11-08T15:07:41.275420Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tiles in backprop: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:11<00:00,  3.87s/it]\n"
     ]
    }
   ],
   "source": [
    "full_gradients = sCNN.backward(image, stream_output.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:52.888465Z",
     "start_time": "2019-11-08T15:07:52.885712Z"
    }
   },
   "outputs": [],
   "source": [
    "sCNN.disable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the gradients of the Conv2d layer to compare with the conventional method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:52.894909Z",
     "start_time": "2019-11-08T15:07:52.890361Z"
    }
   },
   "outputs": [],
   "source": [
    "streaming_conv_gradients = []\n",
    "\n",
    "for i, layer in enumerate(stream_net.modules()):\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        if layer.weight.grad is not None:\n",
    "            streaming_conv_gradients.append(layer.weight.grad.clone()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to conventional method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset the gradients and add hooks to the network to gather the gradients of the intermediate feature maps to compare with streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:52.914145Z",
     "start_time": "2019-11-08T15:07:52.899281Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, layer in enumerate(stream_net.modules()):\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        if layer.weight.grad is not None:\n",
    "            layer.weight.grad.data.zero_()\n",
    "            layer.bias.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:52.933620Z",
     "start_time": "2019-11-08T15:07:52.927035Z"
    }
   },
   "outputs": [],
   "source": [
    "conventional_gradients = []\n",
    "inps = []\n",
    "\n",
    "def save_grad(module, grad_in, grad_out):\n",
    "    global conventional_gradients\n",
    "    conventional_gradients.append(grad_out[0].clone())\n",
    "        \n",
    "for i, layer in enumerate(stream_net.modules()):\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        layer.register_backward_hook(save_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output should be the same as the streaming output, if so, the loss will also be the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:53.259202Z",
     "start_time": "2019-11-08T15:07:52.942475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5616595557, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conventional_output = stream_net(image[None]); conventional_output.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:53.274730Z",
     "start_time": "2019-11-08T15:07:53.265669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal output to streaming\n"
     ]
    }
   ],
   "source": [
    "# NOTE: sometimes output can be slightly bigger \n",
    "# (if tiles do not fit nicely on input image according to output stride)\n",
    "# In that case this check may fail.\n",
    "max_error = torch.abs(stream_output - conventional_output).max().item()\n",
    "\n",
    "if max_error < 1e-7:\n",
    "    print(\"Equal output to streaming\")\n",
    "else:\n",
    "    print(\"NOT equal output to streaming\"),\n",
    "    print(\"error:\", max_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:53.292242Z",
     "start_time": "2019-11-08T15:07:53.283516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8238510002, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.sigmoid(torch.mean(conventional_output)); output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:53.304973Z",
     "start_time": "2019-11-08T15:07:53.299766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-75.3965480581, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(output, target); loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:53.314329Z",
     "start_time": "2019-11-08T15:07:53.306628Z"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the gradients of the feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell concatenates all the intermediate feature map gradients of the tiles to compare it to the feature map gradients calculated during the conventional method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:09:28.505689Z",
     "start_time": "2019-11-08T15:09:28.434644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 feature map gradient - equal to non-streaming\n",
      "2 feature map gradient - equal to non-streaming\n",
      "5 feature map gradient - equal to non-streaming\n",
      "7 feature map gradient - equal to non-streaming\n",
      "10 feature map gradient - equal to non-streaming\n",
      "12 feature map gradient - equal to non-streaming\n"
     ]
    }
   ],
   "source": [
    "equal_eps = 1e-17  # because we are comparing floats the difference is almost never exactly 0\n",
    "\n",
    "if len(sCNN.gradients[stream_net[0]]) == 9:\n",
    "\n",
    "    layer_dict = dict(stream_net.named_modules())\n",
    "    i = -1\n",
    "    for name in layer_dict:\n",
    "        mod = layer_dict[name]\n",
    "        if isinstance(mod, torch.nn.Conv2d) and len(name) > 0:\n",
    "            i += 1\n",
    "\n",
    "            # StreamingCNN streams from top-left to bottom-right. \n",
    "            # First concat the columns, then the rows:\n",
    "            a = torch.cat((sCNN.gradients[mod][0][0], \n",
    "                           sCNN.gradients[mod][1][0], \n",
    "                           sCNN.gradients[mod][2][0]), dim=2)\n",
    "            b = torch.cat((sCNN.gradients[mod][3][0], \n",
    "                           sCNN.gradients[mod][4][0], \n",
    "                           sCNN.gradients[mod][5][0]), dim=2)\n",
    "            c = torch.cat((sCNN.gradients[mod][6][0], \n",
    "                           sCNN.gradients[mod][7][0], \n",
    "                           sCNN.gradients[mod][8][0]), dim=2)\n",
    "            \n",
    "            str_grad = torch.cat((a, b, c), dim=1)\n",
    "\n",
    "            # Compare streaming and conventional:\n",
    "            max_error = torch.abs(str_grad - conventional_gradients[-(i + 1)][0])\n",
    "            max_error = max_error.max().item()\n",
    "            \n",
    "            if max_error < equal_eps:\n",
    "                print(name, \"feature map gradient - equal to non-streaming\")\n",
    "            else:\n",
    "                print(name, \"feature map - NOT equal, max error:\", max_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the gradients of the conv2d layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the gradients of the conv2d layer to compare with normal SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:54.911329Z",
     "start_time": "2019-11-08T15:07:54.905167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv layer 0 \t Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "Conv layer 1 \t Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "Conv layer 2 \t Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "Conv layer 3 \t Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "Conv layer 4 \t Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "Conv layer 5 \t Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "normal_conv_gradients = []\n",
    "j = 0\n",
    "for i, layer in enumerate(stream_net.modules()):\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        if layer.weight.grad is not None:\n",
    "            normal_conv_gradients.append(layer.weight.grad) \n",
    "            print('Conv layer', j, '\\t', layer)\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:54.929868Z",
     "start_time": "2019-11-08T15:07:54.913460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conventional \n",
      "\n",
      "Conv layer 0 \t average gradient size: 0.6411255833746924\n",
      "Conv layer 1 \t average gradient size: 1.2274522598476474\n",
      "Conv layer 2 \t average gradient size: 2.327678320388775\n",
      "Conv layer 3 \t average gradient size: 1.4139776732575264\n",
      "Conv layer 4 \t average gradient size: 2.9613723884791274\n",
      "Conv layer 5 \t average gradient size: 1.9381479236170729\n"
     ]
    }
   ],
   "source": [
    "print('Conventional', '\\n')\n",
    "\n",
    "for i in range(len(streaming_conv_gradients)):\n",
    "    print(\"Conv layer\", i, \"\\t average gradient size:\", \n",
    "          float(torch.mean(torch.abs(streaming_conv_gradients[i].data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:54.962148Z",
     "start_time": "2019-11-08T15:07:54.934375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming \n",
      "\n",
      "Conv layer 0 \t average gradient size: 0.6411255833746914\n",
      "Conv layer 1 \t average gradient size: 1.2274522598476474\n",
      "Conv layer 2 \t average gradient size: 2.3276783203887774\n",
      "Conv layer 3 \t average gradient size: 1.4139776732575267\n",
      "Conv layer 4 \t average gradient size: 2.961372388479127\n",
      "Conv layer 5 \t average gradient size: 1.9381479236170713\n"
     ]
    }
   ],
   "source": [
    "print('Streaming', '\\n')\n",
    "for i in range(len(normal_conv_gradients)):\n",
    "    print(\"Conv layer\", i, \"\\t average gradient size:\", \n",
    "          float(torch.mean(torch.abs(normal_conv_gradients[i].data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T15:07:54.976045Z",
     "start_time": "2019-11-08T15:07:54.969652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv layer 0 \t max difference between kernel gradients: 7.038813976123492e-14\n",
      "Conv layer 1 \t max difference between kernel gradients: 1.021405182655144e-13\n",
      "Conv layer 2 \t max difference between kernel gradients: 6.785683126508957e-13\n",
      "Conv layer 3 \t max difference between kernel gradients: 3.232969447708456e-13\n",
      "Conv layer 4 \t max difference between kernel gradients: 3.126388037344441e-13\n",
      "Conv layer 5 \t max difference between kernel gradients: 1.1368683772161603e-13\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(streaming_conv_gradients)):\n",
    "    diff = torch.abs(streaming_conv_gradients[i].data - normal_conv_gradients[i].data)\n",
    "    max_diff = diff.max()\n",
    "    print(\"Conv layer\", i, \"\\t max difference between kernel gradients:\", \n",
    "          float(max_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the difference of the gradients of the conv2d layers between the methods is (almost) numerically equivalent. The small differences are because of loss of significance with the floating points calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "344px",
    "left": "2614.95px",
    "top": "98.0799px",
    "width": "383px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
