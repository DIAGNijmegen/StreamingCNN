{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emperical experiment for StreamingCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms.functional\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scnn import StreamingCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=10)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compare the networks we want to train deterministically\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('imagenette-320.tgz'):\n",
    "    !wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz\n",
    "    !tar -xvf imagenette-320.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamNet(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(StreamNet, self).__init__(\n",
    "            torch.nn.Conv2d(3, 16, kernel_size=7, stride=2, padding=2), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.AdaptiveMaxPool2d(1)\n",
    "        )\n",
    "\n",
    "        self.classifier = torch.nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_net = StreamNet().cuda()\n",
    "net = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(stream_net) + count_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure streamingSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sCNN = StreamingCNN(stream_net, tile_shape=(1, 3, 160, 160), deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagenetteDataset(object):\n",
    "    def __init__(self, patch_size=320, validation=False, should_normalize=True):\n",
    "        self.folder = Path('imagenette-320/train') if not validation else Path('imagenette-320/val')\n",
    "        self.classes = ['n01440764', 'n02102040', 'n02979186', 'n03000684', 'n03028079',\n",
    "                        'n03394916', 'n03417042', 'n03425413', 'n03445777', 'n03888257']\n",
    "\n",
    "        self.images = []\n",
    "        for cls in self.classes:\n",
    "            cls_images = list(self.folder.glob(cls + '/*.JPEG'))\n",
    "            if validation: self.images += cls_images[0:100]\n",
    "            else: self.images += cls_images[100:200]\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.validation = validation\n",
    "        \n",
    "        self.random_resize = torchvision.transforms.RandomResizedCrop(patch_size)\n",
    "        self.center_resize = torchvision.transforms.CenterCrop(patch_size)\n",
    "        self.should_normalize = should_normalize\n",
    "        self.normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_fname = self.images[index]\n",
    "        image = Image.open(image_fname)\n",
    "        label = image_fname.parent.stem\n",
    "        label = self.classes.index(label)\n",
    "        \n",
    "        if not self.validation: image = self.random_resize(image)\n",
    "        else: image = self.center_resize(image)\n",
    "            \n",
    "        image = torchvision.transforms.functional.to_tensor(image)\n",
    "        if image.shape[0] == 1: image = image.expand(3, 320, 320)\n",
    "        if self.should_normalize: image = self.normalize(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ImagenetteDataset(320)\n",
    "valset = ImagenetteDataset(320, validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\n",
    "visualize_trainset = ImagenetteDataset(320, should_normalize=False)\n",
    "\n",
    "for y in range(3):\n",
    "    for x in range(3):\n",
    "        index = random.randint(0, len(trainset) - 1)\n",
    "        sample = visualize_trainset[index]\n",
    "        ax[y, x].imshow(sample[0].numpy().transpose(1,2,0))\n",
    "        ax[y, x].set_axis_off()\n",
    "\n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod in net.modules():\n",
    "    if isinstance(mod, torch.nn.Conv2d):\n",
    "        torch.nn.init.kaiming_uniform_(mod.weight)\n",
    "        mod.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parameters so that we can start networks from the same initialization\n",
    "original_initialization_stream_net = copy.deepcopy(stream_net.state_dict())\n",
    "original_initialization_net = copy.deepcopy(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(stream_net.parameters()) + list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "stream_losses = []\n",
    "stream_val_losses = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "stream_val_accuracy = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(epochs=100, streaming=False):\n",
    "    # sCNN uses hooks to correct gradients while streaming, \n",
    "    # so we have to disable it when training conventionally\n",
    "    if streaming: sCNN.enable()\n",
    "    else: sCNN.disable()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # we train with checkpointing and gradient accumulation,\n",
    "        # so we need to keep track of the images that will create a batch.\n",
    "        batch_labels = []\n",
    "        batch_predictions = []\n",
    "        batch_images = []\n",
    "\n",
    "        for data in tqdm(trainloader):\n",
    "            for image, label in zip(data[0], data[1]):\n",
    "                # first inference / stream through first network\n",
    "                with torch.no_grad():\n",
    "                    if streaming: first_output = sCNN.forward(image[None])\n",
    "                    else: first_output = stream_net.forward(image[None].cuda())\n",
    "\n",
    "                batch_predictions.append(first_output)\n",
    "                batch_labels.append(label)\n",
    "                batch_images.append(image)\n",
    "                \n",
    "                # gather all predictions as a batch\n",
    "                if len(batch_predictions) == batch_size:\n",
    "                    fmap = torch.cat(batch_predictions, 0)\n",
    "                    fmap.requires_grad = True\n",
    "                    \n",
    "                    labels = torch.cuda.LongTensor(batch_labels)\n",
    "\n",
    "                    # inference final part of network\n",
    "                    second_output = net(fmap)\n",
    "                    \n",
    "                    # backpropagation through final network\n",
    "                    loss = criterion(second_output, labels)\n",
    "                    loss.backward()\n",
    "\n",
    "                    # backpropagation through first network using checkpointing / streaming\n",
    "                    for i, x in enumerate(batch_images):\n",
    "                        if streaming: \n",
    "                            sCNN.backward(x[None], fmap.grad[i][None])\n",
    "                        else:\n",
    "                            first_output = stream_net.forward(x[None].cuda())\n",
    "                            first_output.backward(fmap.grad[i][None])\n",
    "\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # reset batch\n",
    "                    batch_labels = []\n",
    "                    batch_predictions = []\n",
    "                    batch_images = []\n",
    "                    \n",
    "                    running_loss += loss.item()\n",
    "                    \n",
    "        if streaming: stream_losses.append(running_loss / len(trainloader))\n",
    "        else: losses.append(running_loss / len(trainloader))\n",
    "\n",
    "        running_loss = 0\n",
    "        i = 0\n",
    "        accurate = 0\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                for image, label in zip(data[0], data[1]):\n",
    "                    if streaming: first_output = sCNN.forward(image[None])\n",
    "                    else: first_output = stream_net.forward(image[None].cuda())\n",
    "                    second_output = net(first_output)\n",
    "                    \n",
    "                    loss = criterion(second_output, label[None].cuda())\n",
    "                    running_loss += loss.item()\n",
    "                    i += 1\n",
    "                    \n",
    "                    if torch.argmax(torch.softmax(second_output, dim=1)).cpu() == label: \n",
    "                        accurate += 1\n",
    "\n",
    "        if streaming: stream_val_accuracy.append(accurate / i)\n",
    "        else: val_accuracy.append(accurate / i)\n",
    "            \n",
    "        if streaming: stream_val_losses.append(running_loss / i)\n",
    "        else: val_losses.append(running_loss /  i)\n",
    "            \n",
    "        plot_losses()\n",
    "\n",
    "def plot_losses():\n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(20, 5))\n",
    "    ax[0].plot(np.arange(len(losses)), losses, label=\"loss, normal case\")\n",
    "    ax[0].plot(np.arange(len(stream_losses)), stream_losses, label=\"loss, streaming\", color=\"C1\")\n",
    "    ax[1].plot(np.arange(len(losses)), losses, label=\"train loss, normal case\")\n",
    "    ax[1].plot(np.arange(len(val_losses)), val_losses, label=\"validation loss, normal case\")\n",
    "    ax[2].plot(np.arange(len(stream_losses)), stream_losses, label=\"train loss, streaming\", color=\"C1\")\n",
    "    ax[2].plot(np.arange(len(stream_val_losses)), stream_val_losses, label=\"train loss, streaming\", color=\"C2\")\n",
    "    ax[3].plot(np.arange(len(val_accuracy)), val_accuracy, label=\"normal accuracy\")\n",
    "    ax[3].plot(np.arange(len(stream_val_accuracy)), stream_val_accuracy, label=\"streaming accuracy\")\n",
    "    ax[0].legend() \n",
    "    ax[1].legend() \n",
    "    ax[2].legend() \n",
    "    ax[3].legend()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train conventionally (not streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original state\n",
    "stream_net.load_state_dict(original_initialization_stream_net)\n",
    "net.load_state_dict(original_initialization_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dataloaders after re-seed, if we do this in both training cases the order is identical\n",
    "torch.manual_seed(0)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=1)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=4, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(epochs=200, streaming=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original state\n",
    "stream_net.load_state_dict(original_initialization_stream_net)\n",
    "net.load_state_dict(original_initialization_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dataloaders after re-seed\n",
    "torch.manual_seed(0)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=1)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=4, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(epochs=200, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: LSUV initialization as explained in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def data_dependent_init(net, checkpoint_net, dataloader, n_images=20):\n",
    "    with torch.no_grad():\n",
    "        for m in chain(net.modules(), checkpoint_net.modules()):\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                if not hasattr(m, 'iteration'):\n",
    "                    m.running_sum = torch.zeros(m.out_channels).cuda()\n",
    "                    m.running_sqrs = torch.ones(m.out_channels).cuda()\n",
    "                    m.iterator = 0\n",
    "\n",
    "        def init_hook(mod, inpt, outpt):\n",
    "            dims = (0, 2, 3)\n",
    "            nc = outpt.shape[1]\n",
    "            s = outpt.sum(dims, keepdim=False)\n",
    "            ss = (outpt*outpt).sum(dims, keepdim=False)\n",
    "            c = outpt.numel() / nc\n",
    "\n",
    "            mod.running_sum += s\n",
    "            mod.running_sqrs += ss\n",
    "            mod.iterator += c\n",
    "\n",
    "            means = (s / c)\n",
    "            var = (ss / c) - means**2\n",
    "            stds = var.sqrt()\n",
    "\n",
    "            outpt -= means[:, None, None]\n",
    "            outpt /= (stds[:, None, None] + 1e-10)\n",
    "\n",
    "        hooks = []\n",
    "        for mod in chain(net.modules(), checkpoint_net.modules()):\n",
    "            if isinstance(mod, torch.nn.Conv2d):\n",
    "                hook = mod.register_forward_hook(init_hook)\n",
    "                hooks.append(hook)\n",
    "\n",
    "        # first do forward pass to determine running stds average\n",
    "        net.eval()\n",
    "        i = 0\n",
    "        for x, y in dataloader:\n",
    "            if i == n_images: break\n",
    "            for image, label in zip(x, y):\n",
    "                first_output = checkpoint_net.forward(image[None].cuda())\n",
    "                second_output = net(first_output)\n",
    "                i += 1\n",
    "\n",
    "        # correct weights with running stds average\n",
    "        for m in chain(net.modules(), checkpoint_net.modules()):\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                mean = (m.running_sum / m.iterator)\n",
    "                variance = (m.running_sqrs / m.iterator) - mean**2\n",
    "                stds = variance.sqrt() + 1e-10\n",
    "                m.weight.data /= stds[:, None, None, None]\n",
    "                m.bias.data -= mean / stds\n",
    "\n",
    "        for hook in hooks:\n",
    "            hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dependent_init(net, stream_net, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "344px",
    "left": "2614.95px",
    "top": "98.0799px",
    "width": "383px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
